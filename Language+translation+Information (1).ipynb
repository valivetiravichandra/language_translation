{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 style='color:brown;'>---------------Language translation using Deep learning LSTM's----------------</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of contents</h1>\n",
    "\n",
    "<a href='#intro' style=\"text-decoration:none;color:purple;\">1) Intro to Seq2Seq models </a>\n",
    "\n",
    "<a href='#trad' style=\"text-decoration:none;color:purple;\">2) Traditional phrase based statistical translation model</a>\n",
    "\n",
    "<a href='#arch' style=\"text-decoration:none;color:purple;\">3) Architecture of Sequence to Sequence model </a>\n",
    "\n",
    "<a href='#keras' style=\"text-decoration:none;color:purple;\">4) Keras and tensorflow support</a>\n",
    "\n",
    "<a href='#other' style=\"text-decoration:none;color:purple;\">5) Other methods for machine translation</a>\n",
    "\n",
    "<a href='#data' style=\"text-decoration:none;color:purple;\">6) Datasets available for language translation</a>\n",
    "\n",
    "<a href='#code' style=\"text-decoration:none;color:purple;\">7) Code snippet in keras </a>\n",
    "\n",
    "<a href='#transfer' style=\"text-decoration:none;color:purple;\">8) Transfer learning for machine translation</a>\n",
    "\n",
    "<a href='#metric' style=\"text-decoration:none;color:purple;\">9) Evaluation metrics and loss function</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Intro to Seq2Seq models for language translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-sequence (seq2seq) models have enjoyed great success in a variety of tasks such as machine translation, speech recognition, and text summarization.\n",
    "Use of Seq2Seq models in language translation: Converting english text to french language:\n",
    "    ### \"the cat sat on the mat\" -> [Seq2Seq model] -> \"le chat etait assis sur le tapis\"\n",
    "    \n",
    "![title](pictures/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder-decoder architecture – example of a general approach for NMT. An encoder converts a source sentence into a \"meaning\" vector which is passed through a decoder to produce a translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trad'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Traditional method Phrase based translation A.K.A Statistical machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more check out:\n",
    "https://en.wikipedia.org/wiki/Statistical_machine_translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Back in the old days, traditional phrase-based translation systems performed their task by breaking up source sentences into multiple chunks and then translated them phrase-by-phrase. This led to disfluency in the translation outputs and was not quite like how we, humans, translate. We read the entire source sentence, understand its meaning, and then produce a translation. Neural Machine Translation (NMT) mimics that!</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arch'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Architecture of Seq2Seq model:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the basic architecture of Seq2Seq model which has an:\n",
    "\n",
    "1) Encoder which converts input(Source language sentence) into meaning vector(Context).\n",
    "\n",
    "2) Note that here output of encoder is discarded, but returns the internal state as a context to decoder.\n",
    "\n",
    "3) Then decoder translates the Source language to target language by predicting the next output in a sequential manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='keras'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Keras and tensorflow support for seq2seq models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq models can be created both in tensorflow as well as in keras. \n",
    "Steps in creating a Seq2Seq model:\n",
    "\n",
    "1) Prepare encoder input data, decoder input data, decoder target data.\n",
    "\n",
    "2) Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data. \n",
    "\n",
    "3) Decode some sentences to check that the model is working\n",
    "\n",
    "For more information checkout:\n",
    "\n",
    "https://www.tensorflow.org/tutorials/seq2seq\n",
    "\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='other'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Other translation methods available:\n",
    "<ul>\n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Machine_translation\" title=\"Machine translation\">Machine translation</a></li>\n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Rule-based_machine_translation\" title=\"Rule-based machine translation\">Rule-based machine translation</a></li>\n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Transfer-based_machine_translation\" title=\"Transfer-based machine translation\">Transfer-based machine translation</a></li>\n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Interlingual_machine_translation\" title=\"Interlingual machine translation\">Interlingual machine translation</a></li>\n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Statistical_machine_translation\" title=\"Statistical machine translation\">Statistical machine translation</a></li>\n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Example-based_machine_translation\" title=\"Example-based machine translation\">Example-based machine translation</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Datasets available for Neural machine translation:\n",
    "\n",
    "1) Small-scale: English-Vietnamese parallel corpus of TED talks (133K sentence pairs) provided by the IWSLT Evaluation Campaign.\n",
    "\n",
    "2) Large-scale: German-English parallel corpus (4.5M sentence pairs) provided by the WMT Evaluation Campaign.\n",
    "\n",
    "3) Stanford NLP group: https://nlp.stanford.edu/projects/nmt/\n",
    "\n",
    "4) Data sets for various languages for creating base models: http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Code snippet for Seq2Seq model in keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style='color:purple'>\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "#Define an input sequence and process it.\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "#We discard `encoder_outputs` and only keep the states.\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#Set up the decoder, using `encoder_states` as initial state.\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "#We set up our decoder to return full output sequences,\n",
    "and to return internal states as well. We don't use the \n",
    "return states in the training model, but we will use them in inference.\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#Define the model that will turn\n",
    "#`encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transfer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Transfer learning in Machine translation:\n",
    "\n",
    "The encoder-decoder framework for neural\n",
    "machine translation (NMT) has been shown\n",
    "effective in large data scenarios, but is much\n",
    "less effective for low-resource languages. We\n",
    "present a transfer learning method that signifi-\n",
    "cantly improves BLEU scores across a range\n",
    "of low-resource languages. Our key idea is\n",
    "to first train a high-resource language pair\n",
    "(the parent model), then transfer some of the\n",
    "learned parameters to the low-resource pair\n",
    "(the child model) to initialize and constrain\n",
    "training.\n",
    "\n",
    "http://www.aclweb.org/anthology/D16-1163\n",
    "\n",
    "https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A spectacular example of Transfer Learning would be <h3>Google’s Multilingual Neural Machine Translation (GNMT) system.</h3> \n",
    "\n",
    "In GNMT, a single model is trained to translate between language pairs such as English⇄Korean and English⇄Japanese. That is, samples consisting of translation pairs of English⇄Korean and English⇄Japanese are used to train a unified model.\n",
    "\n",
    "![title](pictures/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot translation\n",
    "The GNMT system is said to represent an improvement over the former Google Translate in that it can handle \"zero-shot translation\", that is it directly translates one language into another (for example, Japanese to Korean).[2] Google Translate previously first translated the source language into English and then translated the English into the target language rather than translating directly from one language to another.[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model consists of a deep LSTM network with 8 encoder and 8 decoder layers using residual connections as well as attention connections from the decoder network to the encoder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a single model to translate between any two languages, the model is forced to learn universal features common to all languages. This enables the system to do “Zero-Shot Translation”: the model is able to translate between a language pair for which it hasn’t explicitly seen any training data. In the case of English, Japanese and Korean, a model trained to translate between English⇄Korean and English⇄Japanese is also able to translate between Korean⇄Japanese without any explicit supervised training.\n",
    "![title](pictures/6.png)\n",
    "\n",
    "https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html\n",
    "\n",
    "https://arxiv.org/pdf/1611.04558.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='metric'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Evaluation metics and loss function for Language translation model:\n",
    "\tAutomatic evaluation\n",
    "1\tBLEU\n",
    "\n",
    "2\tNIST\n",
    "\n",
    "3\tWord error rate\n",
    "\n",
    "4\tMETEOR\n",
    "\n",
    "5\tLEPOR\n",
    "\n",
    "Most commonly used metric is the BLEU score, it stands for  (bilingual evaluation understudy).\n",
    "\n",
    "BLEU was one of the first metrics to report high correlation with human judgments of quality. The metric is currently one of the most popular in the field. The central idea behind the metric is that \"the closer a machine translation is to a professional human translation, the better it is\". The metric calculates scores for individual segments, generally sentences—then averages these scores over the whole corpus for a final score. It has been shown to correlate highly with human judgments of quality at the corpus level.\n",
    "\n",
    "BLEU uses a modified form of precision to compare a candidate translation against multiple reference translations. The metric modifies simple precision since machine translation systems have been known to generate more words than appear in a reference text. No other machine translation metric is yet to significantly outperform BLEU with respect to correlation with human judgment across language pairs.\n",
    "\n",
    "<h2>BLEU’s evaluation system requires two inputs: \n",
    "<li>\n",
    " a numerical translation closeness metric, which is then assigned and measured against \n",
    "</li>\n",
    "<li>\n",
    " a corpus of human reference translations.\n",
    "</li>\n",
    "</h2>\n",
    "\n",
    "Loss is the categorical cross entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time taken to train On WMT En→Fr, the training set contains 36M sentence pairs\n",
    "### ---On WMT En→Fr, it takes around 6 days to train a basic model using 96 NVIDIA K80 GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Attention is all you need paper on using attention mechanism on seq2seq models\n",
    "\n",
    "https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](pictures/7.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google neural machine translator architecture:\n",
    "![title](pictures/8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
